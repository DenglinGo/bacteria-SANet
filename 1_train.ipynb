{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "upset-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import ScaleAdaptiveNet\n",
    "from data import spectralloader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,CosineAnnealingLR\n",
    "import torch.nn as nn\n",
    "from utils import Engine,EarlyStop,seed_everything\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-balance",
   "metadata": {},
   "source": [
    "# Training Configuration\n",
    "Example training configureation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sustainable-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'dataset':'reference', #choose dataset in reference,finetune,test,2018clinical and 2019clnical\n",
    "    'device':'cuda:0',\n",
    "    'epoch':1,            #maximun epochs for training,here take 1 epoch as an example\n",
    "    'batch_size':64,\n",
    "    'lr':1e-3,\n",
    "    'wd':1e-5,\n",
    "    'patience':10,        #patience for early stop\n",
    "    'sheduler_type':'ReduceLROnPlateau',\n",
    "    'seed':42,\n",
    "    'num_classes':30,\n",
    "    'accumulation_steps':1,#update parameters after forward and backward for 'accumulation_steps'times\n",
    "    'num_folds':5,\n",
    "    'mssa_mrsa':False,    #if True, the dataloader will only load the MSSA/MRSA data\n",
    "    'antibiotics':False,  #if True, the target of the dataset will be the empriric treatment classes\n",
    "    'fp16':False,         #uing mixed precision for training or not\n",
    "    'save_path':'param/example/' #saving the params of models in the path\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-reconstruction",
   "metadata": {},
   "source": [
    "## Fixed random seed\n",
    "Ensure the reproducibility of the experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broken-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(CFG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-april",
   "metadata": {},
   "source": [
    "## Initialize data-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "graduate-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = spectralloader(\n",
    "    dataset=CFG['dataset'],\n",
    "    batch_size=CFG['batch_size'],\n",
    "    antibiotics=CFG['antibiotics'],\n",
    "    num_folds = CFG['num_folds'],\n",
    "    mssa_mrsa=CFG['mssa_mrsa'],\n",
    "    seed = CFG['seed']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-miracle",
   "metadata": {},
   "source": [
    "## Training Example\n",
    "Training for 1 epochs on fold1.\n",
    "Training and validating by using the tool Class Engine and EarlyStop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-lebanon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold1, Epoch1 : Average train loss is 1.087112, the accuracy rate of trainset is 0.6899 :  74%|███████▍  | 554/750 [00:49<00:17, 11.36it/s]"
     ]
    }
   ],
   "source": [
    "for i in [1]: #replaced with for i in folds:\n",
    "    trainloader = folds[i]['train']\n",
    "    valloader = folds[i]['val']\n",
    "    net = ScaleAdaptiveNet(num_classes=CFG['num_classes']).to(CFG['device'])\n",
    "    #    net.load_state_dict(torch.load('pretrained_model path')) use this if fine tune\n",
    "\n",
    "    lossF = nn.BCEWithLogitsLoss().to(CFG['device']) if CFG['mssa_mrsa'] else nn.CrossEntropyLoss().to(CFG['device'])\n",
    "    optimizer = torch.optim.SGD(net.parameters(),momentum=0.9,lr=CFG['lr'],weight_decay=CFG['wd'])\n",
    "    sheduler = ReduceLROnPlateau(optimizer,mode='max',factor=0.3,\n",
    "                                 verbose=False,threshold=0.001,patience=5) if CFG['sheduler_type']=='ReduceLROnPlateau' else\\\n",
    "        CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "        \n",
    "    es = EarlyStop(patience=CFG['patience'])\n",
    "    for epoch in range(1,CFG['epoch']+1):\n",
    "        Engine.train(i,epoch,trainloader,lossF,\n",
    "                     net,optimizer,CFG['device'],sheduler,CFG['accumulation_steps'],CFG['fp16'],\n",
    "                    CFG['sheduler_type'],CFG['mssa_mrsa'])\n",
    "        val_acc,_ = Engine.evaluate(i,epoch,valloader,lossF,net,CFG['device'],CFG['mssa_mrsa'])\n",
    "        #save the model if val_acc is increasing.\n",
    "        es(val_acc,net,CFG['save_path']+str(i)+'.pth') \n",
    "        #if the val_acc stop increasing for 'patience' epoches, end the training for the fold\n",
    "        if es.early_stop: \n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-buyer",
   "metadata": {},
   "source": [
    "After training, the model with highest validation accuracy on each fold will be saved in CFG['save_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-confidentiality",
   "metadata": {},
   "source": [
    "## Config 1\n",
    "Pretraining on reference dataset for 30 isolate-class task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'dataset':'reference', #choose dataset in reference,finetune,test,2018clinical and 2019clnical\n",
    "    'device':'cuda:0',\n",
    "    'epoch':100,            #maximun epochs for training\n",
    "    'batch_size':64,\n",
    "    'lr':1e-3,\n",
    "    'wd':1e-5,\n",
    "    'patience':10,        #patience for early stop\n",
    "    'sheduler_type':'ReduceLROnPlateau',\n",
    "    'seed':42,\n",
    "    'num_classes':30,\n",
    "    'accumulation_steps':1,#update parameters after forward and backward for 'accumulation_steps'times\n",
    "    'num_folds':5,\n",
    "    'mssa_mrsa':False,    #if True, the dataloader will only load the MSSA/MRSA data\n",
    "    'antibiotics':False,  #if True, the target of the dataset will be the empriric treatment classes\n",
    "    'fp16':False,         #uing mixed precision for training or not\n",
    "    'save_path':'param/30/pretrain/' #saving the params of models to the path\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-slave",
   "metadata": {},
   "source": [
    "## Config 2\n",
    "Finetuning for 30 isolate-class task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'dataset':'finetune', #choose dataset in reference,finetune,test,2018clinical and 2019clnical\n",
    "    'device':'cuda:0',\n",
    "    'epoch':100,            #maximun epochs for training\n",
    "    'batch_size':16,\n",
    "    'lr':1e-4,\n",
    "    'wd':1e-6,\n",
    "    'patience':10,        #patience for early stop\n",
    "    'sheduler_type':'CosineAnnealingLR',\n",
    "    'seed':42,\n",
    "    'num_classes':30,\n",
    "    'accumulation_steps':1,#update parameters after forward and backward for 'accumulation_steps'times\n",
    "    'num_folds':5,\n",
    "    'mssa_mrsa':False,    #if True, the dataloader will only load the MSSA/MRSA data\n",
    "    'antibiotics':False,  #if True, the target of the dataset will be the empriric treatment classes\n",
    "    'fp16':False,         #uing mixed precision for training or not\n",
    "    'save_path':'param/30/finetune/' #saving the params of models to the path\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-brief",
   "metadata": {},
   "source": [
    "## Config 3\n",
    "Pretraining on reference dataset for MSSA/MRSA task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'dataset':'reference', #choose dataset in reference,finetune,test,2018clinical and 2019clnical\n",
    "    'device':'cuda:0',\n",
    "    'epoch':100,            #maximun epochs for training\n",
    "    'batch_size':64,\n",
    "    'lr':1e-3,\n",
    "    'wd':1e-5,\n",
    "    'patience':10,        #patience for early stop\n",
    "    'sheduler_type':'ReduceLROnPlateau',\n",
    "    'seed':42,\n",
    "    'num_classes':1,\n",
    "    'accumulation_steps':1,#update parameters after forward and backward for 'accumulation_steps'times\n",
    "    'num_folds':5,\n",
    "    'mssa_mrsa':True,    #if True, the dataloader will only load the MSSA/MRSA data\n",
    "    'antibiotics':False,  #if True, the target of the dataset will be the empriric treatment classes\n",
    "    'fp16':False,         #uing mixed precision for training or not\n",
    "    'save_path':'param/2/pretrain/' #saving the params of models to the path\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-acceptance",
   "metadata": {},
   "source": [
    "## Config 4\n",
    "Finetuning for MSSA/MRSA task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'dataset':'finetune', #choose dataset in reference,finetune,test,2018clinical and 2019clnical\n",
    "    'device':'cuda:0',\n",
    "    'epoch':100,            #maximun epochs for training\n",
    "    'batch_size':16,\n",
    "    'lr':1e-4,\n",
    "    'wd':1e-6,\n",
    "    'patience':10,        #patience for early stop\n",
    "    'sheduler_type':'CosineAnnealingLR',\n",
    "    'seed':42,\n",
    "    'num_classes':1,\n",
    "    'accumulation_steps':1,#update parameters after forward and backward for 'accumulation_steps'times\n",
    "    'num_folds':5,\n",
    "    'mssa_mrsa':True,    #if True, the dataloader will only load the MSSA/MRSA data\n",
    "    'antibiotics':False,  #if True, the target of the dataset will be the empriric treatment classes\n",
    "    'fp16':False,         #uing mixed precision for training or not\n",
    "    'save_path':'param/2/finetune/' #saving the params of models to the path\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-terrain",
   "metadata": {},
   "source": [
    "## Config 5\n",
    "Pretraining on reference dataset for clinical task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'dataset':'reference', #choose dataset in reference,finetune,test,2018clinical and 2019clnical\n",
    "    'device':'cuda:0',\n",
    "    'epoch':100,            #maximun epochs for training\n",
    "    'batch_size':64,\n",
    "    'lr':1e-3,\n",
    "    'wd':1e-5,\n",
    "    'patience':10,        #patience for early stop\n",
    "    'sheduler_type':'ReduceLROnPlateau',\n",
    "    'seed':42,\n",
    "    'num_classes':8,\n",
    "    'accumulation_steps':1,#update parameters after forward and backward for 'accumulation_steps'times\n",
    "    'num_folds':5,\n",
    "    'mssa_mrsa':False,    #if True, the dataloader will only load the MSSA/MRSA data\n",
    "    'antibiotics':True,  #if True, the target of the dataset will be the empriric treatment classes\n",
    "    'fp16':False,         #uing mixed precision for training or not\n",
    "    'save_path':'param/8/pretrain/' #saving the params of models to the path\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorchenv] *",
   "language": "python",
   "name": "conda-env-.conda-pytorchenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
